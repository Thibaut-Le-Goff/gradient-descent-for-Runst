 <center><h2><ins>Gradient-descent-for-Runst</ins></h2></center>

The program here is the adaptation of Josh Starmer's video:
[![here](https://img.youtube.com/vi/sDv4f4s2SB8&t/0.jpg)](https://www.youtube.com/watch?v=sDv4f4s2SB8&t)

___

As I am blocking at coding the backpropagation algorithme in this video:
[![here](https://img.youtube.com/vi/GKZoOHXGcLo&t/0.jpg)](https://www.youtube.com/watch?v=GKZoOHXGcLo&t)
And as I am indeed going totally bonker I have to step back and rethink what I have learned so far.

To be 100% sure of what is about gradient descent I will try to explain it.

But I will also try to make some hypotheses of what should be add to the gradient descent algorithm to work in a neural network, at least the one in the video mentioned earlier...

<ins>The goal of gradient descent:</ins>\
The goal of gradient descent is to determine the intercept and the slope of a "prediction line", here the line predict the height of someone by giving her or his weight.

But the algorithm need some samples in order to have a grasp of how this line must look, in terme of intercept and slope, those sample are the the given weight of three peoples and their observed height.

First, the algorithm will make an initial guess, those are stored in ```slope_intercept```, there are initialised as two 0 but they can be others values like negative one or even randomly declared.


After his guess, the algorithme will 

In a neural network...

how to fit the line to the data, least square

